{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import scipy.optimize as optim\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import feature_extraction\n",
    "from __future__ import division\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from random import seed, shuffle\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import numpy.core.multiarray\n",
    "import cvxpy\n",
    "import cvxpy as cvx\n",
    "import dccp\n",
    "from dccp.problem import is_dccp\n",
    "import utils as ut\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('../data/compas-scores-two-years.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"age_cat\", \"race\", \"sex\", \"priors_count\", \"c_charge_degree\"] # feature classification\n",
    "cont_feature = [\"priors_count\"] # continuous features \n",
    "sensitive_attrs = [\"race\"]\n",
    "\n",
    "# convert df to np.array\n",
    "data = df.to_dict('list')\n",
    "for k in data.keys():\n",
    "    data[k] = np.array(data[k])\n",
    "\n",
    "# Data Filtering\n",
    "idx = np.logical_and(data[\"days_b_screening_arrest\"]<=30, data[\"days_b_screening_arrest\"]>=-30)\n",
    "idx = np.logical_and(idx, data[\"is_recid\"] != -1)\n",
    "idx = np.logical_and(idx, data[\"c_charge_degree\"] != \"O\") \n",
    "idx = np.logical_and(idx, data[\"score_text\"] != \"NA\")\n",
    "idx = np.logical_and(idx, np.logical_or(data[\"race\"] == \"African-American\", data[\"race\"] == \"Caucasian\")) # regard African-American as 0,Caucasian as 1\n",
    "\n",
    "for k in data.keys():\n",
    "    data[k] = data[k][idx]\n",
    "    \n",
    "# convert class label 0 to -1\n",
    "y = data['two_year_recid']\n",
    "y[y==0] = -1\n",
    "\n",
    "X = np.array([]).reshape(len(y), 0) # empty array with num rows same as num examples, will hstack the features to it\n",
    "x_control = defaultdict(list)\n",
    "\n",
    "feature_names = []\n",
    "for attr in features:\n",
    "    vals = data[attr]\n",
    "    if attr in cont_feature:\n",
    "        vals = [float(v) for v in vals]\n",
    "        vals = preprocessing.scale(vals) # 0 mean and 1 variance  \n",
    "        vals = np.reshape(vals, (len(y), -1)) # convert from 1-d arr to a 2-d arr with one col\n",
    "\n",
    "    else: # for binary categorical variables, the label binarizer uses just one var instead of two\n",
    "        lb = preprocessing.LabelBinarizer()\n",
    "        lb.fit(vals)\n",
    "        vals = lb.transform(vals)\n",
    "\n",
    "        # add to sensitive features dict\n",
    "    if attr in sensitive_attrs:\n",
    "        x_control[attr] = vals\n",
    "\n",
    "\n",
    "    # add to learnable features\n",
    "    X = np.hstack((X, vals))\n",
    "\n",
    "    if attr in cont_feature: # continuous feature, just append the name\n",
    "        feature_names.append(attr)\n",
    "    else: # categorical features\n",
    "        if vals.shape[1] == 1: # binary features that passed through lib binarizer\n",
    "            feature_names.append(attr)\n",
    "        else:\n",
    "            for k in lb.classes_: # non-binary categorical features, need to add the names for each cat\n",
    "                feature_names.append(attr + \"_\" + str(k))\n",
    "\n",
    "\n",
    "# convert the sensitive feature to 1-d array\n",
    "x_control = dict(x_control)\n",
    "for k in x_control.keys():\n",
    "    assert(x_control[k].shape[1] == 1) # make sure that the sensitive feature is binary after one hot encoding\n",
    "    x_control[k] = np.array(x_control[k]).flatten()\n",
    "\n",
    "# sys.exit(1)\n",
    "\n",
    "# permute the date randomly\n",
    "perm = list(range(0,X.shape[0]))\n",
    "shuffle(perm)\n",
    "X = X[perm]\n",
    "y = y[perm]\n",
    "for k in x_control.keys():\n",
    "    x_control[k] = x_control[k][perm]\n",
    "\n",
    "# add intercept to data\n",
    "m,n = X.shape\n",
    "intercept = np.ones(m).reshape(m, 1)\n",
    "X = np.concatenate((intercept,X), axis = 1)\n",
    "\n",
    "feature_names = [\"intercept\"] + feature_names\n",
    "assert(len(feature_names) == X.shape[1])\n",
    "sensitive_attrs = x_control.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_train_test(x_all, y_all, x_control_all, train_fold_size):\n",
    "\n",
    "    split_point = int(round(float(x_all.shape[0]) * train_fold_size))\n",
    "    x_all_train = x_all[:split_point]\n",
    "    x_all_test = x_all[split_point:]\n",
    "    y_all_train = y_all[:split_point]\n",
    "    y_all_test = y_all[split_point:]\n",
    "    x_control_all_train = {}\n",
    "    x_control_all_test = {}\n",
    "    for k in x_control_all.keys():\n",
    "        x_control_all_train[k] = x_control_all[k][:split_point]\n",
    "        x_control_all_test[k] = x_control_all[k][split_point:]\n",
    "\n",
    "    return x_all_train, y_all_train, x_control_all_train, x_all_test, y_all_test, x_control_all_test\n",
    "\n",
    "train_fold_size = 0.5\n",
    "x_train, y_train, x_control_train, x_test, y_test, x_control_test = split_into_train_test(X, y, x_control, train_fold_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_logistic(X):\n",
    "    if X.ndim > 1: raise Exception(\"Array of samples cannot be more than 1-D!\")\n",
    "    out = np.empty_like(X) # same dimensions and data types\n",
    "    idx = X>0\n",
    "    out[idx] = -np.log(1.0 + np.exp(-X[idx]))\n",
    "    out[~idx] = X[~idx] - np.log(1.0 + np.exp(X[~idx]))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic loss\n",
    "def logistic_loss(w, x, y, return_arr=None):\n",
    "    yz = y * np.dot(x,w)\n",
    "    if return_arr == True:\n",
    "        out = -(log_logistic(yz))\n",
    "    else:\n",
    "        out = -np.sum(log_logistic(yz))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_disp_mist(x, y, x_control, loss_function, EPS, cons_params=None):\n",
    "    max_iters = 100 \n",
    "    max_iter_dccp = 50 \n",
    "\n",
    "    num_points, num_features = x.shape\n",
    "    w = cvx.Variable(num_features) # weight vector\n",
    "\n",
    "    # initialize a random value of w\n",
    "    np.random.seed(112233)\n",
    "    w = np.random.rand(x.shape[1])\n",
    "\n",
    "    if cons_params is None: # just train a simple classifier, no fairness constraints\n",
    "        constraints = []\n",
    "    else:\n",
    "        constraints = get_constraint_list_cov(x, y, x_control, cons_params[\"sensitive_attrs_to_cov_thresh\"], cons_params[\"cons_type\"], w)\n",
    "\n",
    "    if loss_function == \"logreg\":\n",
    "        # constructing the logistic loss problem\n",
    "        # loss = sum( cvx.logistic( mul_elemwise(-y, x*w)  )  ) / num_points # convert y to a diagonal matrix for consistent\n",
    "        loss = logistic_loss(w,x,y)\n",
    "\n",
    "    if cons_params is not None:\n",
    "        if cons_params.get(\"take_initial_sol\") is None: # true by default\n",
    "            take_initial_sol = True\n",
    "        elif cons_params[\"take_initial_sol\"] == False:\n",
    "            take_initial_sol = False\n",
    "\n",
    "        if take_initial_sol == True: # get the initial solution\n",
    "            p = cvx.Problem(cvx.Minimize(loss), [])\n",
    "            p.solve()\n",
    "\n",
    "    # construct the cvxpy problem\n",
    "    prob = cvx.Problem(cvx.Minimize(loss), constraints)\n",
    "\n",
    "    try:\n",
    "\n",
    "        tau, mu = 0.005, 1.2 # default dccp parameters\n",
    "        if cons_params is not None: # in case we passed these parameters as a part of dccp constraints\n",
    "            if cons_params.get(\"tau\") is not None: tau = cons_params[\"tau\"]\n",
    "            if cons_params.get(\"mu\") is not None: mu = cons_params[\"mu\"]\n",
    "\n",
    "        prob.solve(method='dccp', tau=tau, mu=mu, tau_max=1e10,\n",
    "                   solver='ECOS', verbose=False, \n",
    "                   feastol=EPS, abstol=EPS, reltol=EPS,feastol_inacc=EPS, abstol_inacc=EPS, reltol_inacc=EPS,\n",
    "                   max_iters=max_iters, max_iter=max_iter_dccp)\n",
    "\n",
    "        \n",
    "        assert(prob.status == \"Converged\" or prob.status == \"optimal\")\n",
    "        \n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        sys.stdout.flush()\n",
    "        sys.exit(1)\n",
    "\n",
    "    # check that the fairness constraint is satisfied\n",
    "    for f_c in constraints:\n",
    "        assert(f_c.value == True) \n",
    "        pass\n",
    "        \n",
    "    w = np.array(w).flatten() # flatten converts it to a 1d array\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_boundary(w, x, s_attr_arr):\n",
    "    distances_boundary = np.zeros(x.shape[0])\n",
    "    if isinstance(w, dict): # if we have separate weight vectors per group\n",
    "        for k in w.keys(): # for each w corresponding to each sensitive group\n",
    "            d = np.dot(x, w[k])\n",
    "            distances_boundary[s_attr_arr == k] = d[s_attr_arr == k] # set this distance only for people with this sensitive attr val\n",
    "    else: # we just learn one w for everyone else\n",
    "        distances_boundary = np.dot(x, w)\n",
    "    return distances_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model, x_train, y_train, x_test, y_test, y_train_predicted, y_test_predicted):\n",
    "    \n",
    "    if model is not None and y_test_predicted is not None:\n",
    "        print (\"Either the model (w) or the predicted labels should be None\")\n",
    "        raise Exception(\"Either the model (w) or the predicted labels should be None\")\n",
    "\n",
    "    if model is not None:\n",
    "        y_test_predicted = np.sign(np.dot(x_test, model))\n",
    "        y_train_predicted = np.sign(np.dot(x_train, model))\n",
    "    def get_accuracy(y, Y_predicted):\n",
    "        correct_answers = (Y_predicted == y).astype(int) # will have 1 when the prediction and the actual label match\n",
    "        accuracy = float(sum(correct_answers)) / float(len(correct_answers))\n",
    "        return accuracy, sum(correct_answers)\n",
    "\n",
    "    train_score, correct_answers_train = get_accuracy(y_train, y_train_predicted)\n",
    "    test_score, correct_answers_test = get_accuracy(y_test, y_test_predicted)\n",
    "\n",
    "    return train_score, test_score, correct_answers_train, correct_answers_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fpr_fnr_sensitive_features(y_true, y_pred, x_control, sensitive_attrs, verbose = False):\n",
    "\n",
    "\n",
    "\n",
    "    # we will make some changes to x_control in this function, so make a copy in order to preserve the origianl referenced object\n",
    "    x_control_internal = deepcopy(x_control)\n",
    "\n",
    "    s_attr_to_fp_fn = {}\n",
    "    \n",
    "    for s in sensitive_attrs:\n",
    "        s_attr_to_fp_fn[s] = {}\n",
    "        s_attr_vals = x_control_internal[s]\n",
    "        if verbose == True:\n",
    "            print (\"||  s  || FPR. || FNR. ||\")\n",
    "        for s_val in sorted(list(set(s_attr_vals))):\n",
    "            s_attr_to_fp_fn[s][s_val] = {}\n",
    "            y_true_local = y_true[s_attr_vals==s_val]\n",
    "            y_pred_local = y_pred[s_attr_vals==s_val]\n",
    "\n",
    "            \n",
    "\n",
    "            acc = float(sum(y_true_local==y_pred_local)) / len(y_true_local)\n",
    "\n",
    "            fp = sum(np.logical_and(y_true_local == -1.0, y_pred_local == +1.0)) # something which is -ve but is misclassified as +ve\n",
    "            fn = sum(np.logical_and(y_true_local == +1.0, y_pred_local == -1.0)) # something which is +ve but is misclassified as -ve\n",
    "            tp = sum(np.logical_and(y_true_local == +1.0, y_pred_local == +1.0)) # something which is +ve AND is correctly classified as +ve\n",
    "            tn = sum(np.logical_and(y_true_local == -1.0, y_pred_local == -1.0)) # something which is -ve AND is correctly classified as -ve\n",
    "\n",
    "            all_neg = sum(y_true_local == -1.0)\n",
    "            all_pos = sum(y_true_local == +1.0)\n",
    "\n",
    "            fpr = float(fp) / float(fp + tn)\n",
    "            fnr = float(fn) / float(fn + tp)\n",
    "            tpr = float(tp) / float(tp + fn)\n",
    "            tnr = float(tn) / float(tn + fp)\n",
    "\n",
    "\n",
    "            s_attr_to_fp_fn[s][s_val][\"fp\"] = fp\n",
    "            s_attr_to_fp_fn[s][s_val][\"fn\"] = fn\n",
    "            s_attr_to_fp_fn[s][s_val][\"fpr\"] = fpr\n",
    "            s_attr_to_fp_fn[s][s_val][\"fnr\"] = fnr\n",
    "\n",
    "            s_attr_to_fp_fn[s][s_val][\"acc\"] = (tp + tn) / (tp + tn + fp + fn)\n",
    "            if verbose == True:\n",
    "                if isinstance(s_val, float): # print the int value of the sensitive attr val\n",
    "                    s_val = int(s_val)\n",
    "                print (\"||  %s  || %0.2f || %0.2f ||\" % (s_val, fpr, fnr))\n",
    "\n",
    "        \n",
    "        return s_attr_to_fp_fn\n",
    "\n",
    "\n",
    "def get_sensitive_attr_constraint_fpr_fnr_cov(model, x_arr, y_arr_true, y_arr_dist_boundary, x_control_arr, verbose=False):\n",
    "\n",
    "    assert(x_arr.shape[0] == x_control_arr.shape[0])\n",
    "    if len(x_control_arr.shape) > 1: # make sure we just have one column in the array\n",
    "        assert(x_control_arr.shape[1] == 1)\n",
    "    if len(set(x_control_arr)) != 2: # non binary attr\n",
    "        raise Exception(\"Non binary attr, fix to handle non bin attrs\")\n",
    "\n",
    "    \n",
    "    arr = []\n",
    "    if model is None:\n",
    "        arr = y_arr_dist_boundary * y_arr_true # simply the output labels\n",
    "    else:\n",
    "        arr = np.dot(model, x_arr.T) * y_arr_true # the product with the weight vector -- the sign of this is the output label\n",
    "    arr = np.array(arr)\n",
    "\n",
    "    s_val_to_total = {ct:{} for ct in [0,1,2]}\n",
    "    s_val_to_avg = {ct:{} for ct in [0,1,2]}\n",
    "    cons_sum_dict = {ct:{} for ct in [0,1,2]} # sum of entities (females and males) in constraints are stored here\n",
    "\n",
    "    for v in set(x_control_arr):\n",
    "        s_val_to_total[0][v] = sum(x_control_arr == v)\n",
    "        s_val_to_total[1][v] = sum(np.logical_and(x_control_arr == v, y_arr_true == -1))\n",
    "        s_val_to_total[2][v] = sum(np.logical_and(x_control_arr == v, y_arr_true == +1))\n",
    "\n",
    "\n",
    "    for ct in [0,1,2]:\n",
    "        s_val_to_avg[ct][0] = s_val_to_total[ct][1] / float(s_val_to_total[ct][0] + s_val_to_total[ct][1]) # N1 / N\n",
    "        s_val_to_avg[ct][1] = 1.0 - s_val_to_avg[ct][0] # N0 / N\n",
    "\n",
    "    \n",
    "    for v in set(x_control_arr):\n",
    "        idx = x_control_arr == v\n",
    "        dist_bound_prod = arr[idx]\n",
    "\n",
    "        cons_sum_dict[0][v] = cvx.sum( np.minimum(0, dist_bound_prod) ) * (s_val_to_avg[0][v] / len(x_arr))\n",
    "        cons_sum_dict[1][v] = cvx.sum( np.minimum(0, ( (1 - y_arr_true[idx]) / 2 ) * dist_bound_prod) ) * (s_val_to_avg[1][v] / sum(y_arr_true == -1))\n",
    "        cons_sum_dict[2][v] = cvx.sum( np.minimum(0, ( (1 + y_arr_true[idx]) / 2 ) * dist_bound_prod) ) * (s_val_to_avg[2][v] / sum(y_arr_true == +1))\n",
    "        \n",
    "    cons_type_to_name = {0:\"ALL\", 1:\"FPR\", 2:\"FNR\"}\n",
    "    for cons_type in [0,1,2]:\n",
    "        cov_type_name = cons_type_to_name[cons_type]    \n",
    "        cov = cons_sum_dict[cons_type][1] - cons_sum_dict[cons_type][0]\n",
    "        if verbose == True:\n",
    "            print( \"Covariance for type '%s' is: %0.7f\" %(cov_type_name, cov))\n",
    "        \n",
    "    return cons_sum_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_encoding(in_arr):\n",
    "    \"\"\"\n",
    "        input: 1-D arr with int vals -- if not int vals, will raise an error\n",
    "        output: m (ndarray): one-hot encoded matrix\n",
    "                d (dict): also returns a dictionary original_val -> column in encoded matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # for k in in_arr:\n",
    "        #if str(type(k)) != \"<type 'numpy.float64'>\" and type(k) != int and type(k) != np.int64:\n",
    "            # print (str(type(k)))\n",
    "            # print (\"************* ERROR: Input arr does not have integer types\")\n",
    "            # return None\n",
    "        \n",
    "    in_arr = np.array(in_arr, dtype=int)\n",
    "    assert(len(in_arr.shape)==1) # no column, means it was a 1-D arr\n",
    "    attr_vals_uniq_sorted = sorted(list(set(in_arr)))\n",
    "    num_uniq_vals = len(attr_vals_uniq_sorted)\n",
    "    if (num_uniq_vals == 2) and (attr_vals_uniq_sorted[0] == 0 and attr_vals_uniq_sorted[1] == 1):\n",
    "        return in_arr, None\n",
    "\n",
    "    \n",
    "    index_dict = {} # value to the column number\n",
    "    for i in range(0,len(attr_vals_uniq_sorted)):\n",
    "        val = attr_vals_uniq_sorted[i]\n",
    "        index_dict[val] = i\n",
    "\n",
    "    out_arr = []    \n",
    "    for i in range(0,len(in_arr)):\n",
    "        tup = np.zeros(num_uniq_vals)\n",
    "        val = in_arr[i]\n",
    "        ind = index_dict[val]\n",
    "        tup[ind] = 1 # set that value of tuple to 1\n",
    "        out_arr.append(tup)\n",
    "\n",
    "    return np.array(out_arr), index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_constraint_list_cov(x_train, y_train, x_control_train, sensitive_attrs_to_cov_thresh, cons_type, w):\n",
    "\n",
    "    \"\"\"\n",
    "    get the list of constraints to be fed to the minimizer\n",
    "    cons_type == 0: means the whole combined misclassification constraint (without FNR or FPR)\n",
    "    cons_type == 1: FPR constraint\n",
    "    cons_type == 2: FNR constraint\n",
    "    cons_type == 4: both FPR as well as FNR constraints\n",
    "    sensitive_attrs_to_cov_thresh: is a dict like {s: {cov_type: val}}\n",
    "    s is the sensitive attr\n",
    "    cov_type is the covariance type. contains the covariance for all misclassifications, FPR and for FNR etc\n",
    "    \"\"\"\n",
    "\n",
    "    constraints = []\n",
    "    for attr in sensitive_attrs_to_cov_thresh.keys():\n",
    "\n",
    "        attr_arr = x_control_train[attr]\n",
    "        attr_arr_transformed, index_dict = get_one_hot_encoding(attr_arr)\n",
    "                \n",
    "        if index_dict is None: # binary attribute, in this case, the attr_arr_transformed is the same as the attr_arr\n",
    "\n",
    "            s_val_to_total = {ct:{} for ct in [0,1,2]} # constrain type -> sens_attr_val -> total number\n",
    "            s_val_to_avg = {ct:{} for ct in [0,1,2]}\n",
    "            cons_sum_dict = {ct:{} for ct in [0,1,2]} # sum of entities (females and males) in constraints are stored here\n",
    "\n",
    "            for v in set(attr_arr):\n",
    "                s_val_to_total[0][v] = sum(x_control_train[attr] == v)\n",
    "                s_val_to_total[1][v] = sum(np.logical_and(x_control_train[attr] == v, y_train == -1)) # FPR constraint so we only consider the ground truth negative dataset for computing the covariance\n",
    "                s_val_to_total[2][v] = sum(np.logical_and(x_control_train[attr] == v, y_train == +1))\n",
    "\n",
    "\n",
    "            for ct in [0,1,2]:\n",
    "                s_val_to_avg[ct][0] = s_val_to_total[ct][1] / float(s_val_to_total[ct][0] + s_val_to_total[ct][1]) # N1/N in our formulation, differs from one constraint type to another\n",
    "                s_val_to_avg[ct][1] = 1.0 - s_val_to_avg[ct][0] # N0/N\n",
    "\n",
    "            \n",
    "            for v in set(attr_arr):\n",
    "\n",
    "                idx = x_control_train[attr] == v                \n",
    "\n",
    "                #################################################################\n",
    "                # #DCCP constraints\n",
    "                # dist_bound_prod = cvx.multiply(y_train[idx],x_train[idx] * w) # y.f(x)\n",
    "                dist_bound_prod = cvx.multiply(y_train[idx],(x_train[idx] * w).shape[1]) # y.f(x)\n",
    "                \n",
    "                cons_sum_dict[0][v] = sum( np.minimum(0, dist_bound_prod) ) * (s_val_to_avg[0][v] / len(x_train)) # avg misclassification distance from boundary\n",
    "                cons_sum_dict[1][v] = sum( np.minimum(0, (cvx.multiply( (1 - y_train[idx])/2.0, dist_bound_prod)) ) ) * (s_val_to_avg[1][v] / sum(y_train == -1)) # avg false positive distance from boundary (only operates on the ground truth neg dataset)\n",
    "                cons_sum_dict[2][v] = sum( np.minimum(0, (cvx.multiply( (1 + y_train[idx])/2.0, dist_bound_prod) ) )) * (s_val_to_avg[2][v] / sum(y_train == +1)) # avg false negative distance from boundary\n",
    "                \n",
    "                #################################################################\n",
    "                \n",
    "            if cons_type == 4:\n",
    "                cts = [1,2]\n",
    "            elif cons_type in [0,1,2]:\n",
    "                cts = [cons_type]\n",
    "            \n",
    "            else:\n",
    "                raise Exception(\"Invalid constraint type\")\n",
    "\n",
    "            #################################################################\n",
    "            #DCCP constraints\n",
    "            for ct in cts:\n",
    "                thresh = abs(sensitive_attrs_to_cov_thresh[attr][ct][1] - sensitive_attrs_to_cov_thresh[attr][ct][0])\n",
    "                constraints.append( cons_sum_dict[ct][1] <= cons_sum_dict[ct][0]  + thresh )\n",
    "                constraints.append( cons_sum_dict[ct][1] >= cons_sum_dict[ct][0]  - thresh )\n",
    "\n",
    "            #################################################################\n",
    "            \n",
    "        else: # otherwise, its a categorical attribute, so we need to set the cov thresh for each value separately\n",
    "            # need to fill up this part\n",
    "            raise Exception(\"Fill the constraint code for categorical sensitive features... Exiting...\")\n",
    "            sys.exit(1)           \n",
    "\n",
    "    return constraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clf_stats(w, x_train, y_train, x_control_train, x_test, y_test, x_control_test, sensitive_attrs):\n",
    "\n",
    "    assert(len(sensitive_attrs) == 1) # ensure that we have just one sensitive attribute\n",
    "    s_attr = list(sensitive_attrs)[0] # for now, lets compute the accuracy for just one sensitive attr\n",
    "\n",
    "    # compute distance from boundary\n",
    "    distances_boundary_train = get_distance_boundary(w, x_train, x_control_train[s_attr])\n",
    "    distances_boundary_test = get_distance_boundary(w, x_test, x_control_test[s_attr])\n",
    "\n",
    "    # compute the class labels\n",
    "    all_class_labels_assigned_train = np.sign(distances_boundary_train)\n",
    "    all_class_labels_assigned_test = np.sign(distances_boundary_test)\n",
    "\n",
    "    train_score, test_score, correct_answers_train, correct_answers_test = check_accuracy(None, x_train, y_train, x_test, y_test, all_class_labels_assigned_train, all_class_labels_assigned_test)\n",
    "   \n",
    "    cov_all_train = {}\n",
    "    cov_all_test = {}\n",
    "    for s_attr in sensitive_attrs:\n",
    "             \n",
    "        print_stats = False # we arent printing the stats for the train set to avoid clutter\n",
    "\n",
    "        # uncomment these lines to print stats for the train fold\n",
    "        print (\"*** Train ***\")\n",
    "        print (\"Accuracy: %0.3f\" % (train_score))\n",
    "        print_stats = True\n",
    "        s_attr_to_fp_fn_train = get_fpr_fnr_sensitive_features(y_train, all_class_labels_assigned_train, x_control_train, sensitive_attrs, print_stats)\n",
    "        cov_all_train[s_attr] = get_sensitive_attr_constraint_fpr_fnr_cov(None, x_train, y_train, distances_boundary_train, x_control_train[s_attr]) \n",
    "        \n",
    "        print (\"\\n\")\n",
    "        print (\"Accuracy: %0.3f\" % (test_score))\n",
    "        print_stats = True # only print stats for the test fold\n",
    "        s_attr_to_fp_fn_test = get_fpr_fnr_sensitive_features(y_test, all_class_labels_assigned_test, x_control_test, sensitive_attrs, print_stats)\n",
    "        cov_all_test[s_attr] = get_sensitive_attr_constraint_fpr_fnr_cov(None, x_test, y_test, distances_boundary_test, x_control_test[s_attr]) \n",
    "        print (\"\\n\")\n",
    "\n",
    "    return train_score, test_score, cov_all_train, cov_all_test, s_attr_to_fp_fn_train, s_attr_to_fp_fn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments for training\n",
    "cons_params = None \n",
    "loss_function = \"logreg\" \n",
    "EPS = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_classifier():\n",
    "    w = train_model_disp_mist(x_train, y_train, x_control_train, loss_function, EPS, cons_params)\n",
    "\n",
    "    train_score, test_score, cov_all_train, cov_all_test, s_attr_to_fp_fn_train, s_attr_to_fp_fn_test = get_clf_stats(w, x_train, y_train, x_control_train, x_test, y_test, x_control_test, sensitive_attrs)\n",
    "\n",
    "    # accuracy and FPR are for the test because we need of for plotting\n",
    "    return w, test_score, s_attr_to_fp_fn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Unconstrained (original) classifier ==\n",
      "*** Train ***\n",
      "Accuracy: 0.467\n",
      "||  s  || FPR. || FNR. ||\n",
      "||  0  || 1.00 || 0.00 ||\n",
      "||  1  || 1.00 || 0.00 ||\n",
      "\n",
      "\n",
      "Accuracy: 0.474\n",
      "||  s  || FPR. || FNR. ||\n",
      "||  0  || 1.00 || 0.00 ||\n",
      "||  1  || 1.00 || 0.00 ||\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "== Constraints on FPR ==\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Cannot evaluate the truth value of a constraint or chain constraints, e.g., 1 >= x >= 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ESTELL~1\\AppData\\Local\\Temp/ipykernel_13256/1871584423.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mcons_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"cons_type\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcons_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tau\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtau\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"mu\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sensitive_attrs_to_cov_thresh\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msensitive_attrs_to_cov_thresh\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mw_cons\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_cons\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_attr_to_fp_fn_test_cons\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\ESTELL~1\\AppData\\Local\\Temp/ipykernel_13256/1910466138.py\u001b[0m in \u001b[0;36mtrain_test_classifier\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_test_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model_disp_mist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_control_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcons_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtrain_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcov_all_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcov_all_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_attr_to_fp_fn_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_attr_to_fp_fn_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_clf_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_control_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_control_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msensitive_attrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ESTELL~1\\AppData\\Local\\Temp/ipykernel_13256/3599983732.py\u001b[0m in \u001b[0;36mtrain_model_disp_mist\u001b[1;34m(x, y, x_control, loss_function, EPS, cons_params)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mconstraints\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mconstraints\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_constraint_list_cov\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_control\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcons_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sensitive_attrs_to_cov_thresh\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcons_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"cons_type\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"logreg\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ESTELL~1\\AppData\\Local\\Temp/ipykernel_13256/857116395.py\u001b[0m in \u001b[0;36mget_constraint_list_cov\u001b[1;34m(x_train, y_train, x_control_train, sensitive_attrs_to_cov_thresh, cons_type, w)\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0mdist_bound_prod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcvx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# y.f(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0mcons_sum_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdist_bound_prod\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ms_val_to_avg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# avg misclassification distance from boundary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m                 \u001b[0mcons_sum_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcvx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdist_bound_prod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ms_val_to_avg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# avg false positive distance from boundary (only operates on the ground truth neg dataset)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[0mcons_sum_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcvx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdist_bound_prod\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ms_val_to_avg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# avg false negative distance from boundary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\cvxpy\\constraints\\constraint.py\u001b[0m in \u001b[0;36m__bool__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[0mRaising\u001b[0m \u001b[0man\u001b[0m \u001b[0merror\u001b[0m \u001b[0mhere\u001b[0m \u001b[0mprevents\u001b[0m \u001b[0mwriting\u001b[0m \u001b[0mchained\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \"\"\"\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_chain_constraints\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;31m# TODO(rileyjmurray): add a function to compute dual-variable violation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\cvxpy\\constraints\\constraint.py\u001b[0m in \u001b[0;36m_chain_constraints\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    214\u001b[0m         \"\"\"Raises an error due to chained constraints.\n\u001b[0;32m    215\u001b[0m         \"\"\"\n\u001b[1;32m--> 216\u001b[1;33m         raise Exception(\n\u001b[0m\u001b[0;32m    217\u001b[0m             (\"Cannot evaluate the truth value of a constraint or \"\n\u001b[0;32m    218\u001b[0m              \"chain constraints, e.g., 1 >= x >= 0.\")\n",
      "\u001b[1;31mException\u001b[0m: Cannot evaluate the truth value of a constraint or chain constraints, e.g., 1 >= x >= 0."
     ]
    }
   ],
   "source": [
    "# Classify the data while optimizing for accuracy\n",
    "\n",
    "# print\n",
    "print (\"== Unconstrained (original) classifier ==\")\n",
    "w_uncons, acc_uncons, s_attr_to_fp_fn_test_uncons = train_test_classifier()\n",
    "# print (\"\\n-----------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "# Now classify such that we optimize for accuracy while achieving perfect fairness\n",
    "# print\n",
    "\n",
    "print (\"\\n\\n== Constraints on FPR ==\") # setting parameter for constraints\n",
    "cons_type = 1 # FPR constraint -- just change the cons_type, the rest of parameters should stay the same\n",
    "tau = 5.0\n",
    "mu = 1.2\n",
    "sensitive_attrs_to_cov_thresh = {\"race\": {0:{0:0, 1:0}, 1:{0:0, 1:0}, 2:{0:0, 1:0}}} # zero covariance threshold, means try to get the fairest solution\n",
    "cons_params = {\"cons_type\": cons_type, \"tau\": tau, \"mu\": mu, \"sensitive_attrs_to_cov_thresh\": sensitive_attrs_to_cov_thresh}\n",
    "\n",
    "w_cons, acc_cons, s_attr_to_fp_fn_test_cons  = train_test_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
