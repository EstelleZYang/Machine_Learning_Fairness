{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "134abb2e",
   "metadata": {},
   "source": [
    "Today we are going to explore the methods of unfairness problem in Machine Learning. Since Propublica organization pointed out that the COMPAS (Correctional Offender Management Profiling for Alternative Sanction) system which is a database containing the criminal history are discrminatory against race and gender.It's analysis shows that Black defandants were often predicted to be at a higher risk of recidivism than they actually were while white defandants were predicted to be less risky. Recidivism is defined as defandants reoffend and get arrested again within two years. And the risk scores evaluated by their systemun results in unfairness between the defandant's recidivism situation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e60f831b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6508ebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run ../lib/LFR_model.ipynb\n",
    "%run ../lib/DM_DM_sen_Model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518f564",
   "metadata": {},
   "source": [
    "Data Preprocessing: From the ProPublica notebook, we removed the rows that \n",
    "1. charge date of a defendants Compas scored crime was not within 30 days from when the person was arrested\n",
    "2. the recidivist flag - is_recid == -1 if we could not find a compas case at all\n",
    "3. those with a c_charge_degree of 'O' which means ordinary traffic offenses. It will not result in Jail time are removed \n",
    "4. since we are only intersted in sample fairness between two races: African-American and Caucasian, we subsets our datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b239172",
   "metadata": {},
   "source": [
    "Here we introduce Learning Fair Representations techiniques to solve unfairness problem, the learning algorithm for fair classification is achieved by formulating fairness as optimization problem of finding good representation. The main idea in this model is to map each individual, represented as a data point in a given input space, to a probability distribution in a new representation space. General speaking, the goal of our model is to learn a good prototype set with the consideration of accuracy and statiscal parity.\n",
    "\n",
    "Reference: Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, Cynthia Dwork, Learning Fair Representations,\n",
    "http://proceedings.mlr.press/v28/zemel13.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fadd360",
   "metadata": {},
   "source": [
    "Before we built the LFR model, we first transform each variable to a learnable indicator value. And according to the research paper, defandants with African-American race are regarded as non-sensitive group , Caucasian defandants are regarded as protected group. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee166c19",
   "metadata": {},
   "source": [
    "As defined in the Learning Fair Representation paper, the Loss function **L = A_z * L_z+ A_x * L_x + A_y * L_y**, where **Ax, Ay, Az** are hyper-parameters governing the trade-off between the system's desire data, And relative **Lz, Lx, Ly** are defined as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d69370",
   "metadata": {},
   "source": [
    "![title](../figs/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e06f421",
   "metadata": {},
   "source": [
    "![title](../figs/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19322953",
   "metadata": {},
   "source": [
    "![title](../figs/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fe5f0c",
   "metadata": {},
   "source": [
    "Therefore we defined the following function to calculate the relative value. And here we use scipy.optimizeo package to minimize our Loss function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a3afcf",
   "metadata": {},
   "source": [
    "We split the protected group and unprotected group first and concatenate them together. Traning sets and testing sets are split proportionally as 6:1, you can see how does each defandants variables are being rescaled and manipulated "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cface0b",
   "metadata": {},
   "source": [
    "The results of the LFR model versus a logistic regression are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "798c6f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the overall test accuracy for LFR is: 47.54%\n",
      "the test accuracy for LFR for sensitive: 48.1%\n",
      "the test accuracy for LFR for nonsensitive: 47.17%\n",
      "the test accuracy for logistic regression is: 67.09%\n",
      "the test accuracy for logistic regression for sensitive is: 64.56%\n",
      "the test accuracy for logistic regression for nonsensitive is: 64.57000000000001%\n"
     ]
    }
   ],
   "source": [
    "return_lfr_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa5c2e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Unconstrained (original) classifier ==\n",
      "\n",
      "\n",
      "Accuracy: 0.649\n",
      "||  s  || FPR. || FNR. ||\n",
      "||  0  || 0.21 || 0.49 ||\n",
      "||  1  || 0.25 || 0.48 ||\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "== Constraints on FPR ==\n",
      "\n",
      "\n",
      "Accuracy: 0.649\n",
      "||  s  || FPR. || FNR. ||\n",
      "||  0  || 0.21 || 0.49 ||\n",
      "||  1  || 0.25 || 0.48 ||\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "return_dm_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77bbde9",
   "metadata": {},
   "source": [
    "### TO DO - add explanation for accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7968005",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook main.ipynb to pdf\n",
      "[NbConvertApp] ERROR | Error while converting 'main.ipynb'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/site-packages/nbconvert/nbconvertapp.py\", line 427, in export_single_notebook\n",
      "    output, resources = self.exporter.from_filename(notebook_filename, resources=resources)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/nbconvert/exporters/exporter.py\", line 181, in from_filename\n",
      "    return self.from_file(f, resources=resources, **kw)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/nbconvert/exporters/exporter.py\", line 199, in from_file\n",
      "    return self.from_notebook_node(nbformat.read(file_stream, as_version=4), resources=resources, **kw)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/nbconvert/exporters/pdf.py\", line 168, in from_notebook_node\n",
      "    latex, resources = super().from_notebook_node(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/nbconvert/exporters/latex.py\", line 72, in from_notebook_node\n",
      "    return super().from_notebook_node(nb, resources, **kw)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/nbconvert/exporters/templateexporter.py\", line 389, in from_notebook_node\n",
      "    output = self.template.render(nb=nb_copy, resources=resources)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 1304, in render\n",
      "    self.environment.handle_exception()\n",
      "  File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 925, in handle_exception\n",
      "    raise rewrite_traceback_stack(source=source)\n",
      "  File \"/usr/local/share/jupyter/nbconvert/templates/latex/index.tex.j2\", line 8, in top-level template code\n",
      "    ((* extends cell_style *))\n",
      "  File \"/usr/local/share/jupyter/nbconvert/templates/latex/style_jupyter.tex.j2\", line 176, in top-level template code\n",
      "    \\prompt{(((prompt)))}{(((prompt_color)))}{(((execution_count)))}{(((extra_space)))}\n",
      "  File \"/usr/local/share/jupyter/nbconvert/templates/latex/base.tex.j2\", line 7, in top-level template code\n",
      "    ((*- extends 'document_contents.tex.j2' -*))\n",
      "  File \"/usr/local/share/jupyter/nbconvert/templates/latex/document_contents.tex.j2\", line 51, in top-level template code\n",
      "    ((*- block figure scoped -*))\n",
      "  File \"/usr/local/share/jupyter/nbconvert/templates/latex/display_priority.j2\", line 5, in top-level template code\n",
      "    ((*- extends 'null.j2' -*))\n",
      "  File \"/usr/local/share/jupyter/nbconvert/templates/latex/null.j2\", line 30, in top-level template code\n",
      "    ((*- block body -*))\n",
      "  File \"/usr/local/share/jupyter/nbconvert/templates/latex/base.tex.j2\", line 206, in block 'body'\n",
      "    ((( super() )))\n",
      "  File \"/usr/local/share/jupyter/nbconvert/templates/latex/null.j2\", line 32, in block 'body'\n",
      "    ((*- block any_cell scoped -*))\n",
      "  File \"/usr/local/share/jupyter/nbconvert/templates/latex/null.j2\", line 85, in block 'any_cell'\n",
      "    ((*- block markdowncell scoped-*)) ((*- endblock markdowncell -*))\n",
      "  File \"/usr/local/share/jupyter/nbconvert/templates/latex/document_contents.tex.j2\", line 68, in block 'markdowncell'\n",
      "    ((( cell.source | citation2latex | strip_files_prefix | convert_pandoc('markdown+tex_math_double_backslash', 'json',extra_args=[]) | resolve_references | convert_pandoc('json','latex'))))\n",
      "  File \"/usr/local/lib/python3.9/site-packages/nbconvert/filters/pandoc.py\", line 24, in convert_pandoc\n",
      "    return pandoc(source, from_format, to_format, extra_args=extra_args)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/nbconvert/utils/pandoc.py\", line 52, in pandoc\n",
      "    check_pandoc_version()\n",
      "  File \"/usr/local/lib/python3.9/site-packages/nbconvert/utils/pandoc.py\", line 100, in check_pandoc_version\n",
      "    v = get_pandoc_version()\n",
      "  File \"/usr/local/lib/python3.9/site-packages/nbconvert/utils/pandoc.py\", line 77, in get_pandoc_version\n",
      "    raise PandocMissing()\n",
      "nbconvert.utils.pandoc.PandocMissing: Pandoc wasn't found.\n",
      "Please check that pandoc is installed:\n",
      "https://pandoc.org/installing.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to pdf main.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c3f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
