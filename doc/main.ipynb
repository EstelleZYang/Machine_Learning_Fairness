{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "134abb2e",
   "metadata": {},
   "source": [
    "Today we are going to explore the methods of unfairness problem in Machine Learning. Since Propublica organization pointed out that the COMPAS (Correctional Offender Management Profiling for Alternative Sanction) system which is a database containing the criminal history are discrminatory against race and gender.It's analysis shows that Black defandants were often predicted to be at a higher risk of recidivism than they actually were while white defandants were predicted to be less risky. Recidivism is defined as defandants reoffend and get arrested again within two years. And the risk scores evaluated by their systemun results in unfairness between the defandant's recidivism situation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e60f831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6508ebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run ../lib/LFR_model.ipynb\n",
    "%run ../lib/DM_DM_sen_Model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518f564",
   "metadata": {},
   "source": [
    "Data Preprocessing: From the ProPublica notebook, we removed the rows that \n",
    "1. charge date of a defendants Compas scored crime was not within 30 days from when the person was arrested\n",
    "2. the recidivist flag - is_recid == -1 if we could not find a compas case at all\n",
    "3. those with a c_charge_degree of 'O' which means ordinary traffic offenses. It will not result in Jail time are removed \n",
    "4. since we are only intersted in sample fairness between two races: African-American and Caucasian, we subsets our datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b239172",
   "metadata": {},
   "source": [
    "Here we introduce Learning Fair Representations techiniques to solve unfairness problem, the learning algorithm for fair classification is achieved by formulating fairness as optimization problem of finding good representation. The main idea in this model is to map each individual, represented as a data point in a given input space, to a probability distribution in a new representation space. General speaking, the goal of our model is to learn a good prototype set with the consideration of accuracy and statiscal parity.\n",
    "\n",
    "Reference: Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, Cynthia Dwork, Learning Fair Representations,\n",
    "http://proceedings.mlr.press/v28/zemel13.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fadd360",
   "metadata": {},
   "source": [
    "Before we built the LFR model, we first transform each variable to a learnable indicator value. And according to the research paper, defandants with African-American race are regarded as non-sensitive group , Caucasian defandants are regarded as protected group. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee166c19",
   "metadata": {},
   "source": [
    "As defined in the Learning Fair Representation paper, the Loss function **L = A_z * L_z+ A_x * L_x + A_y * L_y**, where **Ax, Ay, Az** are hyper-parameters governing the trade-off between the system's desire data, And relative **Lz, Lx, Ly** are defined as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d69370",
   "metadata": {},
   "source": [
    "![title](../figs/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e06f421",
   "metadata": {},
   "source": [
    "![title](../figs/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19322953",
   "metadata": {},
   "source": [
    "![title](../figs/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fe5f0c",
   "metadata": {},
   "source": [
    "Therefore we defined the following function to calculate the relative value. And here we use scipy.optimizeo package to minimize our Loss function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a3afcf",
   "metadata": {},
   "source": [
    "We split the protected group and unprotected group first and concatenate them together. Traning sets and testing sets are split proportionally as 6:1, you can see how does each defandants variables are being rescaled and manipulated "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cface0b",
   "metadata": {},
   "source": [
    "The results of the LFR model versus a logistic regression are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc2117cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the overall test accuracy for LFR is: 47.54%\n",
      "the test accuracy for LFR for sensitive: 48.1%\n",
      "the test accuracy for LFR for nonsensitive: 47.17%\n",
      "the test accuracy for logistic regression is: 67.09%\n",
      "the test accuracy for logistic regression for sensitive is: 64.56%\n",
      "the test accuracy for logistic regression for nonsensitive is: 64.57000000000001%\n"
     ]
    }
   ],
   "source": [
    "return_lfr_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c14e40b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Unconstrained (original) classifier ==\n",
      "\n",
      "\n",
      "Accuracy: 0.649\n",
      "||  s  || FPR. || FNR. ||\n",
      "||  0  || 0.21 || 0.49 ||\n",
      "||  1  || 0.25 || 0.48 ||\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "== Constraints on FPR ==\n",
      "\n",
      "\n",
      "Accuracy: 0.649\n",
      "||  s  || FPR. || FNR. ||\n",
      "||  0  || 0.21 || 0.49 ||\n",
      "||  1  || 0.25 || 0.48 ||\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "return_dm_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e7fe69",
   "metadata": {},
   "source": [
    "### Explanation and Interpretation of Results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5911c5c3-70e3-4c41-9337-fa948678d2da",
   "metadata": {},
   "source": [
    "As noted above the accuracy for a basic logistic model is about 67% overall and 64.6% for sensitive and non-sensitive groups, the accuracy for the LFR model is 47.54% and the accuracy of the DM/DM-sen model was 64.9%. \n",
    "A decrease in accuracy when implementing fair classification is to be expected, particularly in this case where our initial data is an imbalanced dataset. This brings up a couple of problems. There could be sampling bias, which is a situation in which the labeled training data was from a non-representative sample of the population. There might also be label bias which is a situation in which the labeled training data contains samples that were mis-labeled in a way that correlates with being part of a sensitive group.The data can be purposefully unfair and imperfect, making things more fair does not necessarily make things more accurate. There are simply other factors to take into account. \n",
    "\n",
    "The disparate mistreatment model is intended to bridge the gap created by this imbalance and under-representation of minority groups, and inaccuracy of classification of data that is not necessarily linearly seperable. In terms of accuracy it was about on-par with the logistic regression models of the sensitive and nonsensitive groups, if not a little higher but lover than the logistic regression of the data overall. This makes sense, the DM model should have higher accuracy than the LFR model since it accounts for disparate mistreatment but lower accuracy than the baseline model due to possible bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7968005",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to pdf main.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c3f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
