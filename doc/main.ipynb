{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da94ef7",
   "metadata": {},
   "source": [
    "Today we are going to explore the methods of unfairness problem in Machine Learning. Since Propublica organization pointed out that the COMPAS (Correctional Offender Management Profiling for Alternative Sanction) system which is a database containing the criminal history are discrminatory against race and gender.It's analysis shows that Black defandants were often predicted to be at a higher risk of recidivism than they actually were while white defandants were predicted to be less risky. Recidivism is defined as defandants reoffend and get arrested again within two years. And the risk scores evaluated by their systemun results in unfairness between the defandant's recidivism situation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77ae0e18",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca316428",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run LFR_v2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c72eb8",
   "metadata": {},
   "source": [
    "Data Preprocessing: From the ProPublica notebook, we removed the rows that \n",
    "1. charge date of a defendants Compas scored crime was not within 30 days from when the person was arrested\n",
    "2. the recidivist flag - is_recid == -1 if we could not find a compas case at all\n",
    "3. those with a c_charge_degree of 'O' which means ordinary traffic offenses. It will not result in Jail time are removed \n",
    "4. since we are only intersted in sample fairness between two races: African-American and Caucasian, we subsets our datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91793347",
   "metadata": {},
   "source": [
    "Here we introduce Learning Fair Representations techiniques to solve unfairness problem, the learning algorithm for fair classification is achieved by formulating fairness as optimization problem of finding good representation. The main idea in this model is to map each individual, represented as a data point in a given input space, to a probability distribution in a new representation space. General speaking, the goal of our model is to learn a good prototype set with the consideration of accuracy and statiscal parity.\n",
    "\n",
    "Reference: Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, Cynthia Dwork, Learning Fair Representations,\n",
    "http://proceedings.mlr.press/v28/zemel13.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736a93ae",
   "metadata": {},
   "source": [
    "Before we built the LFR model, we first transform each variable to a learnable indicator value. And according to the research paper, defandants with African-American race are regarded as non-sensitive group , Caucasian defandants are regarded as protected group. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83506c01",
   "metadata": {},
   "source": [
    "As defined in the Learning Fair Representation paper, the Loss function **L = A_z * L_z+ A_x * L_x + A_y * L_y**, where **Ax, Ay, Az** are hyper-parameters governing the trade-off between the system's desire data, And relative **Lz, Lx, Ly** are defined as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7daf16",
   "metadata": {},
   "source": [
    "![title](../figs/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b22a1c2",
   "metadata": {},
   "source": [
    "![title](../figs/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a231bd6",
   "metadata": {},
   "source": [
    "![title](../figs/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4485d5",
   "metadata": {},
   "source": [
    "Therefore we defined the following function to calculate the relative value. And here we use scipy.optimizeo package to minimize our Loss function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab97308",
   "metadata": {},
   "source": [
    "We split the protected group and unprotected group first and concatenate them together. Traning sets and testing sets are split proportionally as 6:1, you can see how does each defandants variables are being rescaled and manipulated "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfd164c",
   "metadata": {},
   "source": [
    "The results of the LFR model versus a logistic regression are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2636c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the overall test accuracy for LFR is: 50.06%\n",
      "the test accuracy for LFR for sensitive: 53.2%\n",
      "the test accuracy for LFR for nonsensitive: 48.010000000000005%\n",
      "the test accuracy for logistic regression is: 67.09%\n",
      "the test accuracy for logistic regression for sensitive is: 64.56%\n",
      "the test accuracy for logistic regression for nonsensitive is: 64.57000000000001%\n"
     ]
    }
   ],
   "source": [
    "return_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201be36b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
